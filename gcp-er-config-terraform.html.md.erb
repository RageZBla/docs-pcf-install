---
title: Deploying PAS on GCP Using Terraform
owner: Ops Manager
iaas: GCP
install_type: terraform
---

<%= partial '/pcf/deprecation-notice' %>

<strong><%= modified_date %></strong>

This topic describes how to install and configure Pivotal Application Service (PAS) on Google Cloud Platform (GCP).

Before beginning this procedure, ensure that you have successfully completed the [Configuring BOSH Director on GCP Using Terraform](../om/gcp/config-terraform.html) topic.

<p class="note"><strong>Note:</strong> If you plan to <a href="http://docs.pivotal.io/addon-ipsec/installing.html">install the PCF IPsec add-on</a>, you must do so before installing any other tiles. Pivotal recommends installing IPsec immediately after Ops Manager, and before installing the PAS tile.</p>


## <a id='prereq'></a> Prerequisite

To complete the procedures in this topic, you must have access to the output from when you ran `terraform apply` to create resources for this deployment. You can view this output at any time by running `terraform output`. You use the values in your Terraform output to configure the BOSH Director tile.


## <a id='download-er'></a> Step 1: Download the PAS Tile

1. If you have not already downloaded PAS, log in to [Pivotal Network](https://network.pivotal.io/products/pivotal-cf), and click on **Pivotal Application Service**.

1. From the **Releases** dropdown, select the release to install and choose one of the following:
  1. Click **Pivotal Application Service** to download the PAS `.pivotal` file.
  1. Click **PCF Small Footprint Runtime** to download the Small Footprint Runtime `.pivotal` file. For more information, see [Getting Started with Small Footprint Runtime](./small-footprint.html).


## <a id='add-er'></a> Step 2: Add PAS to Ops Manager

1. Navigate to the Pivotal Cloud Foundry Operations Manager Installation Dashboard.

1. Click **Import a Product** to add the PAS tile to Ops Manager. This may take a while depending on your connection speed.
  <p class="note"><strong>Note:</strong> After you import a tile to Ops Manager, you can view the latest available version of that tile in the Installation Dashboard by enabling the Pivotal Network API. For more information, refer to the [Adding and Deleting Products](./add-delete.html#pivnet-api) topic.</p>

1. On the left, click the plus icon next to the imported PAS product to add it to the Installation Dashboard.

1. Click the newly added PAS tile in the Installation Dashboard.

    <%= image_tag('images/er-tile.png') %>


## <a id='assign-az'></a> Step 3: Assign Availability Zones and Networks

1. Select **Assign AZ and Networks**. These are the Availability Zones that you [create](../om/gcp/config-terraform.html#az) when configuring BOSH Director.

1. Select the first Availability Zone under **Place singleton jobs**. Ops Manager runs any job with a single instance in this Availability Zone.

1. Select all Availability Zones under **Balance other jobs**. Ops Manager balances instances of jobs with more than one instance across the Availability Zones that you specify.
    <p class="note"><strong>Note:</strong> For production deployments, Pivotal recommends at least three Availability Zones for a highly available installation of PAS.</p>

1. From the **Network** dropdown, choose the network you created in [Step 5: Create Networks Page](../om/gcp/config-manual.html#network) of the _Configuring BOSH Director on GCP Using Terraform_ topic.

1. Click **Save**.


## <a id='er-domain-config'></a> Step 5: Configure Domains

1. Select **Domains**.

    <%= image_tag("domains.png") %>

1. Enter the system and application domains that were created by Terraform:
    * **System Domain**: Enter the value of `sys_domain` from your Terraform output.
    * **Apps Domain**: Enter the value of `apps_domain` from your Terraform output.

1. Click **Save**.


## <a id='networking'></a> Step 6: Configure Networking

1. Select **Networking**.

1. Leave the **Router IPs**, **SSH Proxy IPs**, **HAProxy IPs**, and **TCP Router IPs** fields blank. You do not need to complete these fields when deploying PCF to GCP.
   <p class="note"><strong>Note:</strong> You specify load balancers in the **Resource Config** section of PAS later on in the installation process. See the <a href="#config-lb">Configure Load Balancers</a> section of this topic for more information.</p>

1. Under **Certificates and Private Keys for HAProxy and Router**, you must provide an **SSL Certificate and Private Key**. Starting in PCF v.1.12, HAProxy and the Gorouter are enabled to receive TLS communication by default.
  <%= image_tag 'images/networking_haproxy_router_cert_config.png' %><br>
  You can either provide a certificate signed by a Certificate Authority (CA) or click on the **Generate RSA Certificate** link to generate a self-signed certificate in Ops Manager. Ensure the certificate includes `*.YOUR-SYSTEM-DOMAIN`, `*.apps.YOUR-SYSTEM-DOMAIN`, and `*.sys.YOUR-SYSTEM-DOMAIN`.<br/><br/>
  For details about generating certificates in Ops Manager for your wildcard system domains, see [Providing a Certificate for Your SSL/TLS Termination Point](../opsguide/security_config.html#config).</p>
  <p class="note"><strong>Note</strong>: Ensure that you add any certificates that you generate in this pane to your infrastructure load balancer.</p>
1. <%= partial 'router_haproxy_ca' %>
1. <%= partial 'min_tls_version' %>
1. <%= partial 'ip_logging' %>
1. <%= partial 'xforwarded_client_cert_xfcc' %>
1. <%= partial 'haproxy_client_cert_validation' %>
1. <%= partial 'gorouter_client_cert_validation' %>
1. <%= partial 'tls_cipher_suites_router' %>
1. <%= partial 'tls_cipher_suites_haproxy' %>
1. <%= partial 'haproxy_router_tls_forward' %>
1. <%= partial 'haproxy_hsts_config' %>
1. <%= partial 'ssl_verification' %>
1. <%= partial 'http_disable' %>
1. <%= partial 'insecure_cookies' %>
1. <%= partial 'zipkin_enable' %>
1. <%= partial 'enable_router_local_logs' %>
1. By default, the PAS routers handle traffic for applications deployed to an isolation segment created by the PCF Isolation Segment tile. To configure the PAS routers to reject requests for applications within isolation segments, select the **Routers reject requests for Isolation Segments** checkbox.
  <%= image_tag 'isolate-network.png' %>
  Do not enable this option without deploying routers for each isolation segment. See the following topics for more information:
  * [Installing PCF Isolation Segment](../customizing/installing-pcf-is.html)
  * [Sharding Routers for Isolation Segments](../adminguide/routing-is.html#sharding-routers-isolation-segment).
1. <%= partial 'route_services' %>
1. <%= partial 'max_connections_backend' %>
1. <%= partial 'keepalive_connections' %>
1. <%= partial 'router_timeout_backend' %>
1. <%= partial 'frontend_idle_timeout' %>
1. <%= partial 'lb_unhealthy_threshold' %>
1. <%= partial 'lb_healthy_threshold' %>

    <%= image_tag 'images/router_lb_thresholds.png' %>

1. <%= partial 'http_headers_to_log' %>
    ![Http Headers to Log](images/headers_to_log.png)
1. <%= partial 'haproxy_request_max_buffer' %>
1. <%= partial 'protected_domains' %>
1. The **Loggregator Port** defaults to `443` if left blank. Enter a new value to override the default.
1. For **Container Network Interface Plugin**, ensure **Silk** is selected and review the following fields:
    <%= image_tag 'images/cni-silk.png' %>
    <p class="note"><strong>Note:</strong> The <strong>External</strong> option exists to support NSX-T integration for vSphere deployments.</p>
    <ol id="dns-servers">
      <li><%= partial 'app_mtu' %> </li>
      <li><%= partial 'c2c-overlay' %></li>
      <li><%= partial 'c2c-vxlan' %></li>
      <%= partial 'log-app-traffic-enable' %>
    </ol>
  </li>

1. For <strong>DNS Search Domains</strong>, enter DNS search domains for your containers as a comma-separated list. DNS on your containers appends these names to its host names, to resolve them into full domain names.
    <%= image_tag 'images/dns-search-domains.png' %>

1. <%= partial 'networking_database_timeout' %>

1. <%= partial 'tcp_routing_enable' %><a id='tcp-routing-enable'></a>
   1. For GCP, you also need to specify the name of a GCP TCP load balancer in the **LOAD BALANCER** column of TCP Router job of the **Resource Config** screen. You configure this later on in PAS. See the [Configure Load Balancers](#configure-lb) section of this topic.</p>

1. <%= partial 'tcp_routing_disable' %>


1. (Optional) For additional security, enter headers that you want the router to remove from app responses in **Remove Specified HTTP Response Headers**.

1. Click **Save**.


## <a id='application-containers-config'></a> Step 7: Configure Application Containers

<%= partial 'application_container_config' %>

## <a id='er-appdevctrl-config'></a> Step 8: Configure Application Developer Controls

<%= partial 'application_developer_controls' %>

## <a id='app-security'></a> Step 9: Review Application Security Groups

<%= partial 'application_security_group' %>

## <a id='uaa'></a> Step 10: Configure UAA

<%= partial '_uaa-terraform-gcp' %>

## <a id='credhub'></a> Step 11: Configure CredHub

<%= partial '_credhub' %>

## <a id='er-auth-config'></a> Step 12: Configure Authentication and Enterprise SSO

<%= partial 'authsso_config' %>

## <a id='sys-db'></a> Step 13: Configure System Databases

You can configure PAS to use Google Cloud SQL for the databases required by PAS.

<p class="note"><strong>Note:</strong> If you are performing an upgrade, do not modify your existing internal database configuration or you may lose data. You must migrate your existing data first before changing the configuration. Contact Pivotal Support for help. See <a href="upgrading-pcf.html">Upgrading Pivotal Cloud Foundry</a> for additional upgrade information.</p>

### <a id='internal-db'></a> Internal Database Configuration

<p class="note"><strong>Note:</strong> For GCP installations, Pivotal recommends selecting <strong>External</strong> and using Google Cloud SQL. Only use internal MySQL for non-production or test installations on GCP.</p>

<p class="note"><strong>Note:</strong> For Runtime CredHub to work, you must use internal MySQL. See <a href="https://docs.pivotal.io/pivotalcf/2-4/pcf-release-notes/runtime-rn.html#external-credhub">CredHub Database Cannot be External on GCP</a>.</p>

<p class="note"><strong>Note:</strong> Follow these steps if you did not modify your Terraform variables file to deploy an Google CloudSQL instance.</p>

<%= partial 'ert_database_internal' %>

Then proceed to [Step 14: (Optional) Configure Internal MySQL](#internal-mysql) to configure high availability for your internal MySQL databases.

### <a id='create-dbs'></a> External Database Configuration

<p class="note"><strong>Note:</strong> If you use external MySQL, you cannot use Runtime CredHub. See <a href="https://docs.pivotal.io/pivotalcf/2-4/pcf-release-notes/runtime-rn.html#external-credhub">CredHub Database Cannot be External on GCP</a>.</p>

<p class="note"><strong>Note:</strong> Follow these steps if you modified your Terraform variables file to deploy an Google CloudSQL instance.</p>

Pivotal recommends using an external database such as Google Cloud SQL for high availability reasons.

On GCP, you can use Google Cloud SQL and use the automated backup and high availability replica features.

<p class="note"><strong>Note:</strong> To configure an external database for UAA, see the <em>External Database Configuration</em> section of <a href="#uaa">Configure UAA</a>.</p>

<p class="note warning"><strong>WARNING:</strong> Protect whichever database you use in your deployment with a password.</p>

To specify your PAS databases, perform the following steps:

1. Select the **External Databases** option.

1. Complete the following fields:
  * **Hostname**: Enter the value of `sql_db_ip` from your Terraform output.
  * **TCP Port**: Enter `3306`.
  * For the username and password field for each relational database, enter the values of `pas_sql_username` and `pas_sql_password` from your Terraform output.
  <%= image_tag("gcp/gcp-external-db.png") %>

1. Click **Save**.


## <a id='internal-mysql'></a> Step 14: (Optional) Configure Internal MySQL

<%= partial 'mysql_proxy_config' %>

## <a id='filestore'></a> Step 15: Configure File Storage

<%= partial 'max_droplets_packages_config' %>

### <a id='cc-filesystem'></a> Select File Storage Location

<%= partial 'filestore_config_terraform_gcp' %>

For production-level PCF deployments on GCP, Pivotal recommends selecting **External Google Cloud Storage** to minimize downtime. For more information about production-level PCF deployments on GCP, see the [Reference Architecture for Pivotal Cloud Foundry on GCP](../refarch/gcp/gcp_ref_arch.html).

For additional factors to consider when selecting file storage, see the [Considerations for Selecting File Storage in Pivotal Cloud Foundry](../upgrading/configuring.html#file-storage) topic.

#### <a id='internal_filestore'></a> Internal Filestore

<%= partial 'filestore_internal' %>

#### <a id='external_gcp'></a> External Google Cloud Storage

<%= partial 'filestore_gcp_config_terraform' %>

#### <a id='other'></a> Other IaaS Storage Options

[Azure Storage](./azure-er-config.html#external_azure) and [External S3-Compatible File Storage](./pcf-aws-manual-er-config.html#external_s3) are also available as file storage options, but Pivotal does not recommend these for a typical PCF on GCP installation.


## <a id='sys-logging'></a> Step 16: (Optional) Configure System Logging

<%= partial '_system_logging' %>

## <a id='customize-apps-man'></a> Step 17: (Optional) Customize Apps Manager

<%= partial 'custombranding' %>

## <a id='smtp'></a> Step 18: (Optional) Configure Email Notifications

<%= partial 'email-notifs' %>

## <a id='config-autoscaler'></a> Step 19: (Optional) Configure App Autoscaler

<%= partial 'app-autoscaler' %>

## <a id='config-cc'></a> Step 20: Configure Cloud Controller

<%= partial 'cloud-controller' %>

## <a id='config-smoke-test'></a> Step 21: Configure Smoke Tests

<%= partial 'smoketests' %>

## <a id='advanced-features'></a> Step 22: (Optional) Enable Advanced Features

<%= partial 'advanced-features' %>

## <a id='errands'></a> Step 23: Configure Errands

<%= partial 'errands' %>

## <a id='config-lb'></a> Step 24: Configure Load Balancers

1. Click **Resource Config**.

1. Under the **LOAD BALANCERS** column of the **Router** row, enter a comma-delimited list consisting of the values of `ws_router_pool` and `http_lb_backend_name` from your Terraform output. For example, `tcp:pcf-cf-ws,http:pcf-httpslb`. These are the names of the TCP WebSockets and HTTP(S) load balancers for your deployment.
    <p class="note"><strong>Note:</strong> Do not add a space between key/value pairs in the <code>LOAD BALANCER</code> field or it will fail.</p>
    <p class="note"><strong>Note:</strong> If you are using HAProxy in your deployment, then enter the above load balancer values in the <code>LOAD BALANCERS</code> field of the <strong>HAPRoxy</strong> row instead of the <strong>Router</strong> row. For a high availability configuration, scale up the HAProxy job to more than one instance.</p>

1. If you have enabled TCP routing in the [Networking](#tcp-routing-enable) pane, add the value of `tcp_router_pool` from your Terraform output, prepended with `tcp:`, to the **LOAD BALANCERS** column of the **TCP Router** row. For example, `tcp:pcf-cf-tcp`.

1. Enter the name of your SSH load balancer depending on which release you are using.
  * **PAS**: Under the **LOAD BALANCERS** column of the **Diego Brain** row, enter the value of `ssh_router_pool` from your Terraform output, prepended with `tcp:`. For example, `tcp:MY-PCF-ssh-proxy`.
  * **Small Footprint Runtime**: Under the **LOAD BALANCERS** column of the **Control** row, enter the value of `ssh_router_pool` from your Terraform output, prepended with `tcp:`.

1. Verify that the **Internet Connected** checkbox for every job is checked. The terraform templates do not provision a Network Address Translation (NAT) box for Internet connectivity to your VMs so instead they will be provided with ephemeral public IP addresses to allow the jobs to reach the Internet.
   <p class="note"><strong>Note:</strong> If you want to provision a Network Address Translation (NAT) box to provide Internet connectivity to your VMs instead of providing them with public IP addresses, deselect the <strong>Internet Connected</strong> checkboxes. For more information about using NAT in GCP, see the <a href="https://cloud.google.com/compute/docs/networking">GCP documentation</a>.</p>

1. Click **Save**.


## <a id='disable-resources'></a> Step 25: (Optional) Scale Down and Disable Resources

<%= partial 'disable_resources' %>

## <a id='stemcell'></a> Step 26: Download Stemcell

<%= partial "download-stemcell" %>

## <a id='complete'></a> Step 27: Complete the PAS Installation

1. Click the **Installation Dashboard** link to return to the Installation Dashboard.

1. Click **Review Pending Changes**, then **Apply Changes**. The install process generally requires a minimum of 90 minutes to complete. The image shows the Changes Applied window that displays when the installation process successfully completes.

    <%= image_tag("cloudform/ops-manager-complete.png") %>
